{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main resource: https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import flair\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "flair.device = device\n",
    "\n",
    "print(flair.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1+cu110\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-18 17:51:33,967 Reading data from ..\\data\\corpus_10042021\n",
      "2021-04-18 17:51:34,016 Train: ..\\data\\corpus_10042021\\train.csv\n",
      "2021-04-18 17:51:34,017 Dev: ..\\data\\corpus_10042021\\dev.csv\n",
      "2021-04-18 17:51:34,017 Test: ..\\data\\corpus_10042021\\test.csv\n",
      "2021-04-18 17:51:34,033 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 976/976 [00:01<00:00, 896.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-18 17:51:52,771 [b'High', b'Low', b'Medium']\n",
      "Dictionary with 3 tags: High, Low, Medium\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings, TransformerDocumentEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "data_folder = '../data/corpus_10042021'\n",
    "column_name_map = {1: \"text\", 2: \"label_topic\"}\n",
    "\n",
    "# 1. get the corpus\n",
    "corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
    "                                         column_name_map,\n",
    "                                         skip_header=True) \n",
    "\n",
    "# 2. create the label dictionary\n",
    "label_dict = corpus.make_label_dictionary()\n",
    "\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-18 17:52:03,566 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:52:03,568 Model: \"TextClassifier(\n",
      "  (document_embeddings): TransformerDocumentEmbeddings(\n",
      "    (model): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=768, out_features=3, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2021-04-18 17:52:03,570 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:52:03,570 Corpus: \"Corpus: 867 train + 108 dev + 109 test sentences\"\n",
      "2021-04-18 17:52:03,571 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:52:03,572 Parameters:\n",
      "2021-04-18 17:52:03,572  - learning_rate: \"3e-05\"\n",
      "2021-04-18 17:52:03,573  - mini_batch_size: \"4\"\n",
      "2021-04-18 17:52:03,573  - patience: \"3\"\n",
      "2021-04-18 17:52:03,574  - anneal_factor: \"0.5\"\n",
      "2021-04-18 17:52:03,574  - max_epochs: \"10\"\n",
      "2021-04-18 17:52:03,575  - shuffle: \"True\"\n",
      "2021-04-18 17:52:03,575  - train_with_dev: \"False\"\n",
      "2021-04-18 17:52:03,576  - batch_growth_annealing: \"False\"\n",
      "2021-04-18 17:52:03,577 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:52:03,577 Model training base path: \"flair\\transformers\"\n",
      "2021-04-18 17:52:03,578 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:52:03,578 Device: cuda\n",
      "2021-04-18 17:52:03,579 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:52:03,579 Embeddings storage mode: cpu\n",
      "2021-04-18 17:52:03,583 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:52:20,915 epoch 1 - iter 21/217 - loss 1.19461403 - samples/sec: 19.83 - lr: 0.000030\n",
      "2021-04-18 17:52:24,544 epoch 1 - iter 42/217 - loss 1.12949437 - samples/sec: 23.81 - lr: 0.000030\n",
      "2021-04-18 17:52:28,118 epoch 1 - iter 63/217 - loss 1.15264194 - samples/sec: 23.70 - lr: 0.000030\n",
      "2021-04-18 17:52:31,680 epoch 1 - iter 84/217 - loss 1.14448083 - samples/sec: 23.72 - lr: 0.000030\n",
      "2021-04-18 17:52:35,369 epoch 1 - iter 105/217 - loss 1.11512486 - samples/sec: 23.71 - lr: 0.000030\n",
      "2021-04-18 17:52:38,838 epoch 1 - iter 126/217 - loss 1.11426091 - samples/sec: 24.37 - lr: 0.000030\n",
      "2021-04-18 17:52:42,336 epoch 1 - iter 147/217 - loss 1.10587870 - samples/sec: 24.21 - lr: 0.000030\n",
      "2021-04-18 17:52:46,040 epoch 1 - iter 168/217 - loss 1.10500662 - samples/sec: 23.60 - lr: 0.000030\n",
      "2021-04-18 17:52:49,665 epoch 1 - iter 189/217 - loss 1.09497524 - samples/sec: 23.36 - lr: 0.000030\n",
      "2021-04-18 17:52:53,207 epoch 1 - iter 210/217 - loss 1.10011008 - samples/sec: 23.87 - lr: 0.000030\n",
      "2021-04-18 17:52:54,872 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:52:54,873 EPOCH 1 done: loss 1.1028 - lr 0.0000300\n",
      "2021-04-18 17:52:59,860 DEV : loss 1.085874319076538 - score 0.3889\n",
      "2021-04-18 17:53:00,282 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2021-04-18 17:53:01,145 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:53:17,664 epoch 2 - iter 21/217 - loss 0.89872661 - samples/sec: 23.76 - lr: 0.000030\n",
      "2021-04-18 17:53:21,418 epoch 2 - iter 42/217 - loss 1.04192343 - samples/sec: 23.07 - lr: 0.000030\n",
      "2021-04-18 17:53:25,038 epoch 2 - iter 63/217 - loss 0.99030497 - samples/sec: 23.41 - lr: 0.000030\n",
      "2021-04-18 17:53:28,628 epoch 2 - iter 84/217 - loss 0.97182202 - samples/sec: 23.61 - lr: 0.000030\n",
      "2021-04-18 17:53:32,333 epoch 2 - iter 105/217 - loss 0.94474115 - samples/sec: 22.91 - lr: 0.000030\n",
      "2021-04-18 17:53:36,114 epoch 2 - iter 126/217 - loss 0.97531311 - samples/sec: 23.05 - lr: 0.000030\n",
      "2021-04-18 17:53:39,646 epoch 2 - iter 147/217 - loss 0.99652002 - samples/sec: 23.98 - lr: 0.000030\n",
      "2021-04-18 17:53:43,330 epoch 2 - iter 168/217 - loss 0.99709723 - samples/sec: 23.03 - lr: 0.000030\n",
      "2021-04-18 17:53:47,196 epoch 2 - iter 189/217 - loss 1.01130709 - samples/sec: 22.55 - lr: 0.000030\n",
      "2021-04-18 17:53:51,011 epoch 2 - iter 210/217 - loss 1.00766463 - samples/sec: 22.22 - lr: 0.000030\n",
      "2021-04-18 17:53:52,602 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:53:52,603 EPOCH 2 done: loss 1.0167 - lr 0.0000300\n",
      "2021-04-18 17:53:57,956 DEV : loss 1.0972222089767456 - score 0.4259\n",
      "2021-04-18 17:53:58,387 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2021-04-18 17:53:59,303 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:54:16,927 epoch 3 - iter 21/217 - loss 0.63735294 - samples/sec: 21.48 - lr: 0.000030\n",
      "2021-04-18 17:54:20,584 epoch 3 - iter 42/217 - loss 0.68621890 - samples/sec: 23.13 - lr: 0.000030\n",
      "2021-04-18 17:54:24,275 epoch 3 - iter 63/217 - loss 0.61895953 - samples/sec: 23.56 - lr: 0.000030\n",
      "2021-04-18 17:54:27,833 epoch 3 - iter 84/217 - loss 0.66681537 - samples/sec: 23.77 - lr: 0.000030\n",
      "2021-04-18 17:54:31,583 epoch 3 - iter 105/217 - loss 0.66824826 - samples/sec: 23.22 - lr: 0.000030\n",
      "2021-04-18 17:54:35,171 epoch 3 - iter 126/217 - loss 0.66623633 - samples/sec: 23.58 - lr: 0.000030\n",
      "2021-04-18 17:54:38,807 epoch 3 - iter 147/217 - loss 0.66528741 - samples/sec: 23.30 - lr: 0.000030\n",
      "2021-04-18 17:54:42,508 epoch 3 - iter 168/217 - loss 0.68539948 - samples/sec: 22.87 - lr: 0.000030\n",
      "2021-04-18 17:54:46,488 epoch 3 - iter 189/217 - loss 0.66546076 - samples/sec: 21.94 - lr: 0.000030\n",
      "2021-04-18 17:54:50,287 epoch 3 - iter 210/217 - loss 0.66859380 - samples/sec: 22.26 - lr: 0.000030\n",
      "2021-04-18 17:54:51,862 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:54:51,863 EPOCH 3 done: loss 0.6700 - lr 0.0000300\n",
      "2021-04-18 17:54:57,701 DEV : loss 1.4012253284454346 - score 0.4167\n",
      "2021-04-18 17:54:58,132 BAD EPOCHS (no improvement): 1\n",
      "2021-04-18 17:54:58,134 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:55:15,207 epoch 4 - iter 21/217 - loss 0.37452733 - samples/sec: 23.64 - lr: 0.000030\n",
      "2021-04-18 17:55:18,991 epoch 4 - iter 42/217 - loss 0.26923288 - samples/sec: 22.42 - lr: 0.000030\n",
      "2021-04-18 17:55:22,767 epoch 4 - iter 63/217 - loss 0.21834810 - samples/sec: 22.99 - lr: 0.000030\n",
      "2021-04-18 17:55:26,339 epoch 4 - iter 84/217 - loss 0.19313385 - samples/sec: 23.71 - lr: 0.000030\n",
      "2021-04-18 17:55:30,083 epoch 4 - iter 105/217 - loss 0.25247393 - samples/sec: 22.64 - lr: 0.000030\n",
      "2021-04-18 17:55:33,723 epoch 4 - iter 126/217 - loss 0.33163564 - samples/sec: 23.26 - lr: 0.000030\n",
      "2021-04-18 17:55:37,444 epoch 4 - iter 147/217 - loss 0.31836265 - samples/sec: 23.42 - lr: 0.000030\n",
      "2021-04-18 17:55:41,044 epoch 4 - iter 168/217 - loss 0.33948299 - samples/sec: 23.56 - lr: 0.000030\n",
      "2021-04-18 17:55:44,658 epoch 4 - iter 189/217 - loss 0.35919297 - samples/sec: 23.47 - lr: 0.000030\n",
      "2021-04-18 17:55:48,303 epoch 4 - iter 210/217 - loss 0.34326956 - samples/sec: 23.23 - lr: 0.000030\n",
      "2021-04-18 17:55:49,921 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:55:49,922 EPOCH 4 done: loss 0.3490 - lr 0.0000300\n",
      "2021-04-18 17:55:55,045 DEV : loss 3.504218816757202 - score 0.3981\n",
      "2021-04-18 17:55:55,463 BAD EPOCHS (no improvement): 2\n",
      "2021-04-18 17:55:55,464 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:56:12,530 epoch 5 - iter 21/217 - loss 0.14920462 - samples/sec: 22.96 - lr: 0.000030\n",
      "2021-04-18 17:56:16,305 epoch 5 - iter 42/217 - loss 0.12113720 - samples/sec: 22.40 - lr: 0.000030\n",
      "2021-04-18 17:56:19,954 epoch 5 - iter 63/217 - loss 0.08832946 - samples/sec: 23.21 - lr: 0.000030\n",
      "2021-04-18 17:56:23,581 epoch 5 - iter 84/217 - loss 0.13591875 - samples/sec: 23.34 - lr: 0.000030\n",
      "2021-04-18 17:56:27,306 epoch 5 - iter 105/217 - loss 0.11195381 - samples/sec: 23.50 - lr: 0.000030\n",
      "2021-04-18 17:56:31,025 epoch 5 - iter 126/217 - loss 0.10516696 - samples/sec: 22.77 - lr: 0.000030\n",
      "2021-04-18 17:56:34,592 epoch 5 - iter 147/217 - loss 0.09938640 - samples/sec: 23.77 - lr: 0.000030\n",
      "2021-04-18 17:56:38,197 epoch 5 - iter 168/217 - loss 0.10801716 - samples/sec: 24.22 - lr: 0.000030\n",
      "2021-04-18 17:56:41,743 epoch 5 - iter 189/217 - loss 0.10035237 - samples/sec: 23.91 - lr: 0.000030\n",
      "2021-04-18 17:56:45,367 epoch 5 - iter 210/217 - loss 0.09960263 - samples/sec: 23.35 - lr: 0.000030\n",
      "2021-04-18 17:56:46,908 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:56:46,909 EPOCH 5 done: loss 0.0977 - lr 0.0000300\n",
      "2021-04-18 17:56:52,118 DEV : loss 4.503663063049316 - score 0.4074\n",
      "2021-04-18 17:56:52,548 BAD EPOCHS (no improvement): 3\n",
      "2021-04-18 17:56:52,549 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:57:09,186 epoch 6 - iter 21/217 - loss 0.01096321 - samples/sec: 24.14 - lr: 0.000030\n",
      "2021-04-18 17:57:12,803 epoch 6 - iter 42/217 - loss 0.16635178 - samples/sec: 23.43 - lr: 0.000030\n",
      "2021-04-18 17:57:16,440 epoch 6 - iter 63/217 - loss 0.11169903 - samples/sec: 23.89 - lr: 0.000030\n",
      "2021-04-18 17:57:20,381 epoch 6 - iter 84/217 - loss 0.08436707 - samples/sec: 21.52 - lr: 0.000030\n",
      "2021-04-18 17:57:24,352 epoch 6 - iter 105/217 - loss 0.07340783 - samples/sec: 21.35 - lr: 0.000030\n",
      "2021-04-18 17:57:28,046 epoch 6 - iter 126/217 - loss 0.06416077 - samples/sec: 22.92 - lr: 0.000030\n",
      "2021-04-18 17:57:31,792 epoch 6 - iter 147/217 - loss 0.05578123 - samples/sec: 23.31 - lr: 0.000030\n",
      "2021-04-18 17:57:35,368 epoch 6 - iter 168/217 - loss 0.04896297 - samples/sec: 23.73 - lr: 0.000030\n",
      "2021-04-18 17:57:39,063 epoch 6 - iter 189/217 - loss 0.04429047 - samples/sec: 22.94 - lr: 0.000030\n",
      "2021-04-18 17:57:42,817 epoch 6 - iter 210/217 - loss 0.04087827 - samples/sec: 22.59 - lr: 0.000030\n",
      "2021-04-18 17:57:44,508 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:57:44,508 EPOCH 6 done: loss 0.0397 - lr 0.0000300\n",
      "2021-04-18 17:57:49,864 DEV : loss 4.749303817749023 - score 0.3611\n",
      "Epoch     6: reducing learning rate of group 0 to 1.5000e-05.\n",
      "2021-04-18 17:57:50,304 BAD EPOCHS (no improvement): 4\n",
      "2021-04-18 17:57:50,305 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:58:07,975 epoch 7 - iter 21/217 - loss 0.00005716 - samples/sec: 22.57 - lr: 0.000015\n",
      "2021-04-18 17:58:11,809 epoch 7 - iter 42/217 - loss 0.02508441 - samples/sec: 22.68 - lr: 0.000015\n",
      "2021-04-18 17:58:15,513 epoch 7 - iter 63/217 - loss 0.01732075 - samples/sec: 22.88 - lr: 0.000015\n",
      "2021-04-18 17:58:19,306 epoch 7 - iter 84/217 - loss 0.01361163 - samples/sec: 22.36 - lr: 0.000015\n",
      "2021-04-18 17:58:23,050 epoch 7 - iter 105/217 - loss 0.01099351 - samples/sec: 23.26 - lr: 0.000015\n",
      "2021-04-18 17:58:26,669 epoch 7 - iter 126/217 - loss 0.00919458 - samples/sec: 23.43 - lr: 0.000015\n",
      "2021-04-18 17:58:30,263 epoch 7 - iter 147/217 - loss 0.00821168 - samples/sec: 23.58 - lr: 0.000015\n",
      "2021-04-18 17:58:33,966 epoch 7 - iter 168/217 - loss 0.00723041 - samples/sec: 22.90 - lr: 0.000015\n",
      "2021-04-18 17:58:37,637 epoch 7 - iter 189/217 - loss 0.00644124 - samples/sec: 23.72 - lr: 0.000015\n",
      "2021-04-18 17:58:41,516 epoch 7 - iter 210/217 - loss 0.00580736 - samples/sec: 21.88 - lr: 0.000015\n",
      "2021-04-18 17:58:43,189 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:58:43,190 EPOCH 7 done: loss 0.0056 - lr 0.0000150\n",
      "2021-04-18 17:58:49,191 DEV : loss 4.91359281539917 - score 0.3426\n",
      "2021-04-18 17:58:49,759 BAD EPOCHS (no improvement): 1\n",
      "2021-04-18 17:58:49,760 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:59:07,854 epoch 8 - iter 21/217 - loss 0.00004449 - samples/sec: 22.00 - lr: 0.000015\n",
      "2021-04-18 17:59:11,533 epoch 8 - iter 42/217 - loss 0.00005226 - samples/sec: 23.02 - lr: 0.000015\n",
      "2021-04-18 17:59:15,253 epoch 8 - iter 63/217 - loss 0.00007012 - samples/sec: 22.78 - lr: 0.000015\n",
      "2021-04-18 17:59:19,189 epoch 8 - iter 84/217 - loss 0.00006566 - samples/sec: 22.17 - lr: 0.000015\n",
      "2021-04-18 17:59:23,012 epoch 8 - iter 105/217 - loss 0.00128662 - samples/sec: 22.13 - lr: 0.000015\n",
      "2021-04-18 17:59:26,735 epoch 8 - iter 126/217 - loss 0.00110840 - samples/sec: 22.74 - lr: 0.000015\n",
      "2021-04-18 17:59:30,845 epoch 8 - iter 147/217 - loss 0.00099285 - samples/sec: 21.24 - lr: 0.000015\n",
      "2021-04-18 17:59:34,736 epoch 8 - iter 168/217 - loss 0.00087618 - samples/sec: 21.80 - lr: 0.000015\n",
      "2021-04-18 17:59:38,420 epoch 8 - iter 189/217 - loss 0.00078391 - samples/sec: 23.00 - lr: 0.000015\n",
      "2021-04-18 17:59:42,286 epoch 8 - iter 210/217 - loss 0.00070797 - samples/sec: 22.63 - lr: 0.000015\n",
      "2021-04-18 17:59:43,894 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 17:59:43,895 EPOCH 8 done: loss 0.0007 - lr 0.0000150\n",
      "2021-04-18 17:59:49,871 DEV : loss 4.895306587219238 - score 0.3519\n",
      "2021-04-18 17:59:50,324 BAD EPOCHS (no improvement): 2\n",
      "2021-04-18 17:59:50,325 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 18:00:07,893 epoch 9 - iter 21/217 - loss 0.00010072 - samples/sec: 23.07 - lr: 0.000015\n",
      "2021-04-18 18:00:11,585 epoch 9 - iter 42/217 - loss 0.00012150 - samples/sec: 22.89 - lr: 0.000015\n",
      "2021-04-18 18:00:15,300 epoch 9 - iter 63/217 - loss 0.00010203 - samples/sec: 22.79 - lr: 0.000015\n",
      "2021-04-18 18:00:19,149 epoch 9 - iter 84/217 - loss 0.00008725 - samples/sec: 22.69 - lr: 0.000015\n",
      "2021-04-18 18:00:22,932 epoch 9 - iter 105/217 - loss 0.00007373 - samples/sec: 22.42 - lr: 0.000015\n",
      "2021-04-18 18:00:26,648 epoch 9 - iter 126/217 - loss 0.00006346 - samples/sec: 22.78 - lr: 0.000015\n",
      "2021-04-18 18:00:30,509 epoch 9 - iter 147/217 - loss 0.00006347 - samples/sec: 21.98 - lr: 0.000015\n",
      "2021-04-18 18:00:34,609 epoch 9 - iter 168/217 - loss 0.00005864 - samples/sec: 21.28 - lr: 0.000015\n",
      "2021-04-18 18:00:38,311 epoch 9 - iter 189/217 - loss 0.00005899 - samples/sec: 22.90 - lr: 0.000015\n",
      "2021-04-18 18:00:42,053 epoch 9 - iter 210/217 - loss 0.00005566 - samples/sec: 23.28 - lr: 0.000015\n",
      "2021-04-18 18:00:43,688 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 18:00:43,689 EPOCH 9 done: loss 0.0001 - lr 0.0000150\n",
      "2021-04-18 18:00:49,374 DEV : loss 5.066765785217285 - score 0.3889\n",
      "2021-04-18 18:00:49,822 BAD EPOCHS (no improvement): 3\n",
      "2021-04-18 18:00:49,823 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 18:01:07,745 epoch 10 - iter 21/217 - loss 0.00014885 - samples/sec: 22.58 - lr: 0.000015\n",
      "2021-04-18 18:01:11,390 epoch 10 - iter 42/217 - loss 0.00007963 - samples/sec: 23.19 - lr: 0.000015\n",
      "2021-04-18 18:01:14,970 epoch 10 - iter 63/217 - loss 0.00006056 - samples/sec: 23.64 - lr: 0.000015\n",
      "2021-04-18 18:01:18,666 epoch 10 - iter 84/217 - loss 0.00006199 - samples/sec: 23.55 - lr: 0.000015\n",
      "2021-04-18 18:01:22,247 epoch 10 - iter 105/217 - loss 0.00005339 - samples/sec: 23.69 - lr: 0.000015\n",
      "2021-04-18 18:01:25,735 epoch 10 - iter 126/217 - loss 0.00004799 - samples/sec: 24.28 - lr: 0.000015\n",
      "2021-04-18 18:01:29,387 epoch 10 - iter 147/217 - loss 0.00004519 - samples/sec: 23.90 - lr: 0.000015\n",
      "2021-04-18 18:01:32,993 epoch 10 - iter 168/217 - loss 0.00004182 - samples/sec: 23.46 - lr: 0.000015\n",
      "2021-04-18 18:01:36,628 epoch 10 - iter 189/217 - loss 0.00004141 - samples/sec: 23.33 - lr: 0.000015\n",
      "2021-04-18 18:01:40,138 epoch 10 - iter 210/217 - loss 0.00006165 - samples/sec: 24.11 - lr: 0.000015\n",
      "2021-04-18 18:01:41,788 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 18:01:41,789 EPOCH 10 done: loss 0.0001 - lr 0.0000150\n",
      "2021-04-18 18:01:47,044 DEV : loss 5.047904014587402 - score 0.3704\n",
      "Epoch    10: reducing learning rate of group 0 to 7.5000e-06.\n",
      "2021-04-18 18:01:47,482 BAD EPOCHS (no improvement): 4\n",
      "2021-04-18 18:01:48,349 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-18 18:01:48,350 Testing using best model ...\n",
      "2021-04-18 18:01:48,352 loading file flair\\transformers\\best-model.pt\n",
      "2021-04-18 18:02:01,809 \t0.4862\n",
      "2021-04-18 18:02:01,809 \n",
      "Results:\n",
      "- F-score (micro) 0.4862\n",
      "- F-score (macro) 0.4333\n",
      "- Accuracy 0.4862\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High     0.4545    0.3704    0.4082        27\n",
      "         Low     0.7778    0.1944    0.3111        36\n",
      "      Medium     0.4615    0.7826    0.5806        46\n",
      "\n",
      "   micro avg     0.4862    0.4862    0.4862       109\n",
      "   macro avg     0.5646    0.4491    0.4333       109\n",
      "weighted avg     0.5643    0.4862    0.4489       109\n",
      " samples avg     0.4862    0.4862    0.4862       109\n",
      "\n",
      "2021-04-18 18:02:01,810 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.4862,\n",
       " 'dev_score_history': [0.3889,\n",
       "  0.4259,\n",
       "  0.4167,\n",
       "  0.3981,\n",
       "  0.4074,\n",
       "  0.3611,\n",
       "  0.3426,\n",
       "  0.3519,\n",
       "  0.3889,\n",
       "  0.3704],\n",
       " 'train_loss_history': [1.1028372417397214,\n",
       "  1.016748195694339,\n",
       "  0.670027196905168,\n",
       "  0.3489884364888169,\n",
       "  0.09774403547540265,\n",
       "  0.03974058585199527,\n",
       "  0.00562146532445417,\n",
       "  0.0006854604820291186,\n",
       "  5.6424556055403085e-05,\n",
       "  5.987358741580452e-05],\n",
       " 'dev_loss_history': [1.085874319076538,\n",
       "  1.0972222089767456,\n",
       "  1.4012253284454346,\n",
       "  3.504218816757202,\n",
       "  4.503663063049316,\n",
       "  4.749303817749023,\n",
       "  4.91359281539917,\n",
       "  4.895306587219238,\n",
       "  5.066765785217285,\n",
       "  5.047904014587402]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. initialize document embedding by passing list of word embeddings\n",
    "document_embeddings = TransformerDocumentEmbeddings('bert-base-uncased', fine_tune=True)\n",
    "\n",
    "# 5. create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
    "\n",
    "# 6. initialize the text classifier trainer\n",
    "trainer = ModelTrainer(classifier, corpus, optimizer=Adam)\n",
    "\n",
    "# 7. start the training\n",
    "trainer.train('./flair/transformers',\n",
    "              learning_rate=3e-5, # use very small learning rate\n",
    "              mini_batch_size=4,\n",
    "              mini_batch_chunk_size=2, # optionally set this if transformer is too much for your machine\n",
    "              max_epochs=10) # terminate after 10 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
